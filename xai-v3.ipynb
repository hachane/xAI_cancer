{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8047044,"sourceType":"datasetVersion","datasetId":4741449},{"sourceId":8523489,"sourceType":"datasetVersion","datasetId":5027954},{"sourceId":8573870,"sourceType":"datasetVersion","datasetId":5102287},{"sourceId":8620332,"sourceType":"datasetVersion","datasetId":5065871}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.0. Hyperparameters","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nimport pandas as pd\nfrom IPython.display import display, HTML\ncuda = True if torch.cuda.is_available() else False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'cuda: {cuda}')\n!pip install watermark --quiet\n%load_ext watermark\n%watermark -a 'Le Hoang' -u -d -v -p torch,numpy","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:48.780523Z","iopub.execute_input":"2024-06-06T13:47:48.780915Z","iopub.status.idle":"2024-06-06T13:47:58.494547Z","shell.execute_reply.started":"2024-06-06T13:47:48.780832Z","shell.execute_reply":"2024-06-06T13:47:58.493492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set as {seed}\")\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.496556Z","iopub.execute_input":"2024-06-06T13:47:58.496974Z","iopub.status.idle":"2024-06-06T13:47:58.507166Z","shell.execute_reply.started":"2024-06-06T13:47:58.496915Z","shell.execute_reply":"2024-06-06T13:47:58.506376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"postfix_tr = '_tr'\npostfix_te = '_val'\n\ndataset_name = 'tcga-gbm-methxgexcnv-2000-3-omics'\ndata_folder = f'/kaggle/input/{dataset_name}/TCGA_GBM_METHxGExCNV_2000x2000x2000_MinMaxScaler'\nmodel_folder = '/kaggle/working/models'\ntrain_file = f'/kaggle/input/{dataset_name}/main_mogonet.py'","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.508816Z","iopub.execute_input":"2024-06-06T13:47:58.509113Z","iopub.status.idle":"2024-06-06T13:47:58.514831Z","shell.execute_reply.started":"2024-06-06T13:47:58.509083Z","shell.execute_reply":"2024-06-06T13:47:58.514020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subtypes\nimport json\nloc_file_json_id_omic = data_folder + '/1/dct_index_subtype.json'\nwith open(loc_file_json_id_omic) as file_json_id_omic:\n    dct_LABEL_MAPPING_NAME = json.load(file_json_id_omic)\n    # dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\nLABEL_MAPPING_NAME = dct_LABEL_MAPPING_NAME.values()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.516216Z","iopub.execute_input":"2024-06-06T13:47:58.516499Z","iopub.status.idle":"2024-06-06T13:47:58.535799Z","shell.execute_reply.started":"2024-06-06T13:47:58.516473Z","shell.execute_reply":"2024-06-06T13:47:58.535009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\nrseed = 42\n\nnum_models=4\nidx_list = list(range(1,num_models+1))\nview_list = [1,2,3]\n\n# Hyperparameters\nnum_epoch_pretrain = 900\nnum_epoch= 1500\nlr_e_pretrain = 2e-5\nlr_e = 1e-4\nlr_c = 1e-3\ndim_he_list=[250,300,150]\npatience=200\n\n# num_epoch_pretrain = 700\n# num_epoch= 900\n# lr_e_pretrain = 1.5e-5\n# lr_e = 1.5e-5\n# lr_c = 5e-4\n# dim_he_list=[500, 200, 250]\n# patience=250\n\nretrain=False\nbool_using_early_stopping = True\nverbose = False\nprint_hyper = False\nRun_MOGONET = True","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.536862Z","iopub.execute_input":"2024-06-06T13:47:58.537126Z","iopub.status.idle":"2024-06-06T13:47:58.542924Z","shell.execute_reply.started":"2024-06-06T13:47:58.537099Z","shell.execute_reply":"2024-06-06T13:47:58.542102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data size\nfor fold_id in [3]:\n    print(f'idx data: {fold_id}')\n    tmp = list(LABEL_MAPPING_NAME)\n    label_files = ['tr', 'te', 'val']\n    dict = {\n        'tr': 'Train set',\n        'te': 'Test set',\n        'val': 'Validation set'\n    }\n    \n    \n    for label_file in label_files:\n#         print(dict[label_file])\n        df = pd.read_csv(f'{data_folder}/{fold_id}/labels_{label_file}.csv', header=None, names=['featname'])\n        feature_counts = df['featname'].value_counts().sort_index()\n        \n        print(f'{dict[label_file]}')\n                \n        # Create an HTML table for the feature counts\n        html_table = '<table><tr><th>Label</th><th>Count</th></tr>'\n        for feature, count in feature_counts.items():\n            html_table += f'<tr><td>{tmp[feature]}</td><td>{count}</td></tr>'\n        html_table += '</table>'\n\n        # Display the HTML table\n        display(HTML(html_table))\n    print('\\n', '*'*100)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.544056Z","iopub.execute_input":"2024-06-06T13:47:58.544335Z","iopub.status.idle":"2024-06-06T13:47:58.601893Z","shell.execute_reply.started":"2024-06-06T13:47:58.544308Z","shell.execute_reply":"2024-06-06T13:47:58.601014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(f'/kaggle/input/{dataset_name}/')\n# print(sys.path)\n\nfrom models import init_model_dict\nfrom utils import load_model_dict\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom train_test import prepare_trte_data, gen_trte_adj_mat, test_epoch\n\nnum_view = len(view_list)\nnum_class = len(LABEL_MAPPING_NAME)\nif num_class == 2:\n    adj_parameter = 2\n    dim_he_list = [200,200,100]\nif num_class > 2:\n    adj_parameter = 10\n#     dim_he_list = [400,400,200]\ndim_hvcdn= pow(num_class,num_view)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:58.603251Z","iopub.execute_input":"2024-06-06T13:47:58.603651Z","iopub.status.idle":"2024-06-06T13:47:59.391043Z","shell.execute_reply.started":"2024-06-06T13:47:58.603610Z","shell.execute_reply":"2024-06-06T13:47:59.390308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.1. TRAINING","metadata":{}},{"cell_type":"code","source":"if retrain:\n    model_folder = '/kaggle/working/models'\n    for fold_id in idx_list:\n        print(f'idx data: {fold_id}')\n\n        data_folder_idx = data_folder + f'/{fold_id}'\n        model_folder_idx = model_folder + f'/{fold_id}'\n\n        if not bool_using_early_stopping:\n            patience=None\n        \n        \n        !python '{train_file}' '{rseed}' '{data_folder_idx}' '{postfix_tr}' '{postfix_te}' '{model_folder_idx}' '{view_list}' '{num_epoch_pretrain}' '{num_epoch}' '{lr_e_pretrain}' '{lr_e}' '{lr_c}' '{bool_using_early_stopping}' '{verbose}' '{print_hyper}' '{dim_he_list}' '{patience}'\n\n        print('*'*100)\nelse:\n    model_folder = f'/kaggle/input/{dataset_name}/models'","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:47:59.393426Z","iopub.execute_input":"2024-06-06T13:47:59.393691Z","iopub.status.idle":"2024-06-06T13:50:03.913183Z","shell.execute_reply.started":"2024-06-06T13:47:59.393664Z","shell.execute_reply":"2024-06-06T13:50:03.912169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.2. Load trained models and check accuracy phase 1","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\nfor fold_id in idx_list:\n    print(f'idx data: {fold_id}')\n    \n    data_folder_idx = data_folder + f'/{fold_id}'\n    model_folder_idx = model_folder + f'/{fold_id}'\n    \n    for eval_in in ['_tr','_val', '_te']:\n        print(f'*Eval in: {eval_in}')\n        \n\n        data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder_idx, view_list, postfix_tr, \n                                                                                eval_in)\n        adj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)\n\n        num_view = len(view_list)\n        dim_list = [x.shape[1] for x in data_tr_list]\n        dim_hvcdn = pow(num_class,num_view)\n        model_dict = init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hvcdn)\n        for m in model_dict:\n            if cuda:\n                model_dict[m].cuda()\n        model_dict = load_model_dict(model_folder_idx, model_dict)\n\n\n        te_prob = test_epoch(data_trte_list, adj_te_list, trte_idx[\"te\"], model_dict)\n        te_acc = accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n        if num_class == 2:\n            te_f1 = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))\n            te_auc = roc_auc_score(labels_trte[trte_idx[\"te\"]], te_prob[:,1])\n        else:\n            te_f1_weighted = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='weighted')\n            te_f1_macro = f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='macro')\n\n        print(\"Test ACC: {:.3f}\".format(te_acc))\n        if num_class == 2:\n            print(\"Test F1: {:.3f}\".format(te_f1))\n            print(\"Test AUC: {:.3f}\".format(te_auc))\n        else:\n            print(\"Test F1 weighted: {:.3f}\".format(te_f1_weighted))\n            print(\"Test F1 macro: {:.3f}\".format(te_f1_macro))\n        \n        print()\n    print('*'*100)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:50:03.915756Z","iopub.execute_input":"2024-06-06T13:50:03.916298Z","iopub.status.idle":"2024-06-06T13:50:24.397890Z","shell.execute_reply.started":"2024-06-06T13:50:03.916234Z","shell.execute_reply":"2024-06-06T13:50:24.396905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODULE 2: Integrated Gradient","metadata":{}},{"cell_type":"code","source":"import torch\nfrom scipy.stats import mode\nimport torch.nn.functional as F\nbiomarkers_folder = '/kaggle/working/biomarkers/' + 'TCGA_GBM_METHxGExCNV_2000x2000x2000_MinMaxScaler'\npostfix_tr = '_tr'\npostfix_te = '_val'","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:50:24.398975Z","iopub.execute_input":"2024-06-06T13:50:24.399250Z","iopub.status.idle":"2024-06-06T13:50:24.403932Z","shell.execute_reply.started":"2024-06-06T13:50:24.399223Z","shell.execute_reply":"2024-06-06T13:50:24.402986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.1. Baselines for IG","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Load dict mapping name of {int_id_label:label_name} that int_id_label start from 0 and {int_id_omic:omic_name} that int_id_omic start from 1\nloc_file_json_id_label = data_folder + '/1/dct_index_subtype.json'\nloc_file_json_id_omic = data_folder + '/1/dict_id_omics.json'\n\nwith open(loc_file_json_id_label) as file_json_id_label:\n    dct_LABEL_MAPPING_NAME = json.load(file_json_id_label)\n    dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\nprint(\"\\n{int_id_label:label_name} = \", dct_LABEL_MAPPING_NAME)\n\nwith open(loc_file_json_id_omic) as file_json_id_omic:\n    dct_OMIC_MAPPING_NAME = json.load(file_json_id_omic)\n    dct_OMIC_MAPPING_NAME = {int(k): v for k,v in dct_OMIC_MAPPING_NAME.items()} # convert str number key to int\nprint(\"\\n{int_id_omic:omic_name} = \", dct_OMIC_MAPPING_NAME)\n\n\n\n###############\n\n# may be fixed\nn_folds = num_models # fold id start from 1\ntype_data = 'tr' # fixed\nfold_start_id = 1\nlist_fold_id = list(range(n_folds+fold_start_id)[fold_start_id:])\n\n\ndict_fold_dict_baseline = {}\n\n# SPECIAL NEED TO CURRENTLY NEED TO MANUAL CONFIG\n# GE first then CNA | based on characteristic of omic \"limit\" domain knowledge\nlst_special_baseline_name = ['micro_mean__zero__micro_mean', 'macro_mean__zero__macro_mean'] # based on micro_means, zeros and SPLIT by __ without 's'\nlst_special_baseline_name_true_zero_cna = ['micro_mean__true_zero__micro_mean', 'macro_mean__true_zero__macro_mean']\n\nfor fold_id in list_fold_id:\n    data_folder_idx = data_folder + f'/{fold_id}'\n    print(f'\\n*Using data of fold_id= \"{fold_id}\" with data type = \"{type_data}\" for each omic (total \"{len(dct_OMIC_MAPPING_NAME)}\" type(s) of omic) in folder \"{data_folder_idx}\" to create baseline')\n    ##############\n    tmp_label = pd.read_csv(os.path.join(data_folder_idx, f'labels_{type_data}.csv'), header=None)\n\n    tmp_dict_omic_pd_data = {id_omic: pd.read_csv(os.path.join(data_folder_idx, f'{str(id_omic)}_{type_data}.csv'), header=None) for id_omic in dct_OMIC_MAPPING_NAME.keys()}\n    tmp_dict_omic_shape = {id_omic: tmp_dict_omic_pd_data[id_omic].shape[1] for id_omic in dct_OMIC_MAPPING_NAME.keys()} # number of features each omic\n    \n    tmp_dict_omic_pd_data_w_label = {id_omic: tmp_dict_omic_pd_data[id_omic].copy(deep=True) for id_omic in dct_OMIC_MAPPING_NAME.keys()}\n    for id_omic in tmp_dict_omic_pd_data_w_label.keys():\n        tmp_dict_omic_pd_data_w_label[id_omic]['subtype'] = tmp_label[0]\n\n    tmp_dict_baseline= {}\n    \n    #############################\n    tmp_dict_baseline['zeros']= tuple(torch.zeros((1,tmp_dict_omic_shape[id_omic])\n                                              , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n    tmp_dict_baseline['micro_means']= tuple(torch.tensor(tmp_dict_omic_pd_data[id_omic].mean(axis=0).values.reshape(1,-1)\n                                                         , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n    tmp_dict_baseline['macro_means']= tuple(torch.tensor(tmp_dict_omic_pd_data_w_label[id_omic].groupby('subtype').mean().mean(axis=0).values.reshape(1,-1)\n                                                         , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n    ##########################################################\n    \n    #############################\n    for baseline_name in lst_special_baseline_name:\n        \n        list_ordered_base_baseline = baseline_name.split('__')\n        assert len(list_ordered_base_baseline) == len(dct_OMIC_MAPPING_NAME.keys()), 'Number of base baseline need equal to num of omic'\n        \n        tmp_dict_baseline[baseline_name]= tuple(tmp_dict_baseline[base_baseline_name + 's'][idx].detach().clone() for idx, base_baseline_name in enumerate(list_ordered_base_baseline))    \n    ##########################################################\n    \n    \n    for baseline_name in lst_special_baseline_name_true_zero_cna:\n        list_ordered_base_baseline = baseline_name.split('__')\n        assert len(list_ordered_base_baseline) == len(dct_OMIC_MAPPING_NAME.keys()), 'Number of base baseline need equal to num of omic'\n        \n        tmp_dict_baseline[baseline_name]= tuple(tmp_dict_baseline[base_baseline_name + 's'][idx].detach().clone() if base_baseline_name != 'true_zero' else torch.full(tmp_dict_baseline['zeros'][idx].detach().clone().shape, 0.5) for idx, base_baseline_name in enumerate(list_ordered_base_baseline)) \n\n    #############################\n    tmp_dict_baseline['dict_default_micro_means']= {}\n    \n    tmp_list_of_list_exclude_cursor_label_id = [sorted(list(set(dct_LABEL_MAPPING_NAME.keys()) - set([label_id]))) for label_id in dct_LABEL_MAPPING_NAME.keys()]\n    tmp_dict_baseline['dict_default_macro_means']= {}\n    \n    for label_id in dct_LABEL_MAPPING_NAME.keys():\n        tmp_dict_baseline['dict_default_macro_means'][label_id] = tuple(\n            torch.tensor(\n                tmp_dict_omic_pd_data_w_label[id_omic].groupby('subtype').mean().loc[tmp_list_of_list_exclude_cursor_label_id[label_id]].mean(axis=0).values.reshape(1,-1)\n                ,dtype=torch.float32\n            ) for id_omic in dct_OMIC_MAPPING_NAME.keys()\n        )\n        \n        tmp_dict_baseline['dict_default_micro_means'][label_id] = tuple(\n            torch.tensor(\n                tmp_dict_omic_pd_data_w_label[id_omic][\n                    tmp_dict_omic_pd_data_w_label[id_omic]['subtype'] != label_id # filter to exclude row have label is {label_id}\n                ].loc[:,tmp_dict_omic_pd_data_w_label[id_omic].columns != 'subtype' # then filter to exclude column of label before cal mean for all columns\n                     ].mean(axis=0).values.reshape(1,-1)\n                ,dtype=torch.float32\n            ) for id_omic in dct_OMIC_MAPPING_NAME.keys()\n        )\n    ##########################################################\n     \n    # Final assign\n    dict_fold_dict_baseline[fold_id] = tmp_dict_baseline\n    \n    print(f'For fold_id = \"{fold_id}\" Done create all baseline type/with name:\\n\\tin {list(tmp_dict_baseline.keys())}')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:50:24.405182Z","iopub.execute_input":"2024-06-06T13:50:24.405492Z","iopub.status.idle":"2024-06-06T13:50:28.080281Z","shell.execute_reply.started":"2024-06-06T13:50:24.405462Z","shell.execute_reply":"2024-06-06T13:50:28.079333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2. Calculate and Sort IG's score of each feature from: train data + trained model","metadata":{}},{"cell_type":"code","source":"!pip install captum --quiet\nfrom captum.attr import IntegratedGradients","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:50:28.081501Z","iopub.execute_input":"2024-06-06T13:50:28.081797Z","iopub.status.idle":"2024-06-06T13:50:35.612792Z","shell.execute_reply.started":"2024-06-06T13:50:28.081766Z","shell.execute_reply":"2024-06-06T13:50:35.611846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\ndef cal_feat_imp(num_models, type_base_line ='zeros',_n_steps=100, type_data='tr'):\n    \n    loc_file_json_id_omic = data_folder + '/1/dct_index_subtype.json'\n    with open(loc_file_json_id_omic) as file_json_id_omic:\n        dct_LABEL_MAPPING_NAME = json.load(file_json_id_omic)\n        # dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\n    LABEL_MAPPING_NAME = dct_LABEL_MAPPING_NAME.values() \n    \n    # Initialize an empty dictionary to store aggregated df_attr for each subtype\n    aggregated_rank_scores = {subtype: pd.Series(dtype=float) for subtype in LABEL_MAPPING_NAME}\n    \n#     dct_feat_imp_by_models = {}\n    for idx_model in range(1,num_models+1):\n        data_folder_idx = data_folder + f'/{idx_model}'\n        model_folder_idx = model_folder + f'/{idx_model}'\n        added_softmax = True\n        \n        def preprocessing_data(tup_tensor_test_data):\n    \n            data_tr_list = []\n            data_te_list = []\n\n            for i in view_list:\n                data_tr_list.append(torch.tensor(np.loadtxt(os.path.join(data_folder_idx, str(i)+\"_tr.csv\"), delimiter=','),dtype=torch.float32))\n                data_te_list.append(tup_tensor_test_data[i-1])\n                if cuda:\n                    data_tr_list[i-1] = data_tr_list[i-1].to(device)\n                    data_te_list[i-1] = data_te_list[i-1].to(device)            \n\n            # num train's records, test's records\n            num_tr = data_tr_list[0].shape[0]\n            num_te = data_te_list[0].shape[0]\n\n            # idx\n            trte_idx = {}\n            trte_idx[\"tr\"] = list(range(num_tr))\n            trte_idx[\"te\"] = list(range(num_tr, (num_tr+num_te)))\n\n            # num of views or num of omics\n            num_view = len(view_list)\n            data_tensor_list = []\n            for i in range(num_view):\n                data_tensor_list.append(torch.cat((data_tr_list[i], data_te_list[i]), axis=0))\n                if cuda:\n                    data_tensor_list[i] = data_tensor_list[i].to(device)\n\n            data_train_list = []\n            data_trte_list = []\n            for i in range(len(data_tensor_list)):\n                data_train_list.append(data_tensor_list[i][trte_idx[\"tr\"]].clone())\n\n                tup_seq_data = (data_tensor_list[i][trte_idx[\"tr\"]].clone(), data_tensor_list[i][trte_idx[\"te\"]].clone())\n                data_trte_list.append(\n                    torch.cat(tup_seq_data,axis=0)\n                )\n            return data_train_list, data_trte_list, trte_idx\n    \n\n        dct_feat_imp_by_models = {}\n        # loading dataset\n        _data_list=[]\n        _label = np.loadtxt(os.path.join(data_folder_idx, f\"labels_{type_data}.csv\"), delimiter=',').astype(int)\n\n        for i in view_list:\n            _data_loc = os.path.join(data_folder_idx, str(i)+ f\"_{type_data}.csv\")\n            _data_list.append(np.loadtxt(_data_loc, delimiter=','))\n        _tensor_data_list = tuple(torch.tensor(np_arr,dtype=torch.float32).to(device) for np_arr in _data_list)\n\n        gene_name = []\n        for v in view_list:\n            df = pd.read_csv(os.path.join(data_folder_idx, str(v)+\"_featname.csv\"), header=None)    \n            gene_name.extend(df[0].values.tolist())\n            # DONE USE THIS: gene_name.extend(df[0].str.split(r'\\|').str[0].values.tolist()) # only used when want combined same features that first part before | symbol if exist\n        print(f'\\t+ Total features: {len(gene_name)}')\n        \n        ## init models dict\n        data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder_idx, view_list, postfix_tr='_tr', postfix_te='_val')\n        dim_list= [x.shape[1] for x in data_tr_list]\n\n        model_dict = init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hvcdn)\n        # ---- Done\n        \n        ## load models weights\n        print(f'\\t+ Load \"{model_folder_idx}\"\" model\\'s weight')\n        model_dict = load_model_dict(model_folder_idx, model_dict)\n        \n        #---------------------------------------------------------------------\n        \n        \n        \n        def custom_logit_predictor(*tup_tensor_data):\n            for m in model_dict:\n                if cuda:\n                    model_dict[m].to(device)#cuda()\n                model_dict[m].eval()\n            tup_tensor_data = tuple(tensor_data.to(device) if cuda else tensor_data for tensor_data in tup_tensor_data)\n            data_tr_list, data_trte_list, trte_idx = preprocessing_data(tup_tensor_data)\n            _, adj_trte_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)\n\n            ci_list = []\n            num_view = len(view_list)\n            # print(num_view)\n            for i in range(num_view):\n                ci_list.append(model_dict[\"C{:}\".format(i+1)](model_dict[\"E{:}\".format(i+1)](data_trte_list[i],adj_trte_list[i])))\n            if num_view >= 2:\n                c = model_dict[\"C\"](ci_list)    \n            else:\n                c = ci_list[0]\n            c = c[trte_idx[\"te\"],:]\n\n            if added_softmax:\n                c = F.softmax(c, dim=1)\n            return c\n\n        #----------------------------------------\n        start_time=datetime.now()\n        input=_tensor_data_list\n        ig = IntegratedGradients(custom_logit_predictor)\n        \n        number_of_samples = len(_label)\n        \n        # Calculate attribute scores by batch data to avoid running out of memory\n        # 200 is the maximum (approximate) number of samples that will not cause run out of memory with this TCGA BRCA data (on Train data)\n        max_samples_per_batch = 100\n        # Get list end of index to split data into batches\n        list_end_index = [max_samples_per_batch*times \n                          for times in range(1,int(np.ceil(number_of_samples/max_samples_per_batch)))\n                         ] + [number_of_samples]\n\n        # Calculate attribute score:\n        attr = {}\n        for subtype_idx, subtype in enumerate(LABEL_MAPPING_NAME):\n            #----------------------------------------\n            start = 0\n            print(f'\\n\\t\\t<> Calculate attribution scores with subtype: \"{subtype}\":')\n            \n            \n            if type_base_line[:4] == 'dict':\n                print(f'\\t\\t Type baseline:\\n\\t\\t  [{type_base_line}] -> Special baseline for each subtype')\n                baseline = dict_fold_dict_baseline[idx_model][type_base_line][subtype_idx]\n            else:\n                print(f'\\t\\t Type baseline:\\n\\t\\t  [{type_base_line}]')\n                baseline = dict_fold_dict_baseline[idx_model][type_base_line]\n            if cuda:\n                baseline = tuple(tensor_i.cuda() for tensor_i in baseline)\n            print(f'\\t\\t Model pred score:\\n\\t\\t  {custom_logit_predictor(*baseline).detach().cpu().numpy()} -> SUM={torch.sum(custom_logit_predictor(*baseline)).detach().cpu().numpy()}')\n        \n            for end in list_end_index:\n                print(f'\\t\\t\\t:samples from iloc {start} to {end}:')\n\n                input_tensor = tuple(input_omic[start:end].requires_grad_() for input_omic in input)\n\n                attr_temp, delta_temp = ig.attribute(input_tensor,\n                                                     baselines=baseline,\n                                                     target= subtype_idx, return_convergence_delta=True,\n                                                    n_steps=_n_steps)\n                # concatenate genes attribute score for multi-omics data\n                attr_temp = np.concatenate(tuple(attr_temp[idx_atr].detach().cpu().numpy() for idx_atr in range(len(attr_temp))), axis=1)\n                if start == 0:\n                    attr[subtype] =  attr_temp\n                else:\n                    attr[subtype] = np.concatenate((attr[subtype],attr_temp),axis=0)\n                start=end\n\n#                 print(f'\\t\\t\\t: [Delta temp: min={delta_temp.min()}, max={delta_temp.max()}, mean={delta_temp.mean()}, std={delta_temp.std()}]\\n')\n\n        \n        # ------------------------------\n        df_attr = {}\n        # Build dataframe and rename gene id to gene name\n        for subtype in LABEL_MAPPING_NAME:\n            df_attr[subtype] = pd.DataFrame(attr[subtype], columns=gene_name)\n\n        # Take mean all column (gene name) that have same name\n        for subtype in LABEL_MAPPING_NAME:\n            df_attr[subtype] = df_attr[subtype].groupby(by=df_attr[subtype].columns, axis=1).mean()\n            \n        y_interpret = _label\n        for idx_sub, subtype in enumerate(LABEL_MAPPING_NAME):\n            temp_score = df_attr[subtype].loc[np.where((y_interpret == idx_sub))[0],:].mean(axis=0).abs()\n#             print(temp_score.shape)\n            aggregated_rank_scores[subtype] = aggregated_rank_scores[subtype].add(temp_score, fill_value=0)\n#             print(aggregated_rank_scores[subtype].index.shape, aggregated_rank_scores[subtype].index.unique().shape)\n    \n    \n    matrix_top_threshold = pd.DataFrame()\n    for subtype in LABEL_MAPPING_NAME:\n        temp_score = aggregated_rank_scores[subtype]\n        rank_score_by_subtype = temp_score.sort_values(ascending=False)\n#         print(rank_score_by_subtype)\n\n        if len(LABEL_MAPPING_NAME) == 2:\n            # Binary Classifier\n            threshold_inner = THRESHOLD\n            count_inner = rank_score_by_subtype.iloc[:threshold_inner].index.nunique()\n            while count_inner < THRESHOLD:\n                threshold_inner += 1\n                count_inner = rank_score_by_subtype.iloc[:threshold_inner].index.nunique()\n\n            temp = rank_score_by_subtype.iloc[:threshold_inner].index.to_series()\n            matrix_top_threshold[subtype] = temp[~temp.duplicated(keep='first')].reset_index(drop=True)\n        else:\n            # Multi-class Classifier\n            matrix_top_threshold[subtype] = rank_score_by_subtype.iloc[:].index.tolist()\n    \n    combined_column = matrix_top_threshold.values.flatten()\n    res = pd.DataFrame(combined_column, columns=['gene_name'])\n    \n    print('\\t=>Result (Here print out the top 30 biomakers with highest values):')\n    end_time=datetime.now()\n    print(f'\\tRun in {end_time-start_time}')\n    display(res.head(30))\n    return res","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:54:10.741301Z","iopub.execute_input":"2024-06-06T13:54:10.741673Z","iopub.status.idle":"2024-06-06T13:54:10.782635Z","shell.execute_reply.started":"2024-06-06T13:54:10.741636Z","shell.execute_reply":"2024-06-06T13:54:10.781791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def export_biomarker_file(list_concerned_type_baseline, list_concerned_type_data, list_concerned_num_models, biomarkers_folder, _n_steps=100):\n    list_biomakers_file_loc = []\n    print('_'*100)\n    print(f'Biomarker Discovery for {biomarkers_folder.split(\"/\")[-1]} cohort:')\n    pd_results = {}\n    for type_base_line in list_concerned_type_baseline:\n        pd_results[type_base_line] = {}\n        for type_data in list_concerned_type_data:\n            pd_results[type_base_line][type_data] = {}\n            for num_models in list_concerned_num_models:\n                print(f'- using \"{num_models}\" models')\n                print(f'- using \"{type_base_line}\" baseline with \"{type_data}\" dataset')\n                \n                pd_results[type_base_line][type_data][num_models] = cal_feat_imp(num_models=num_models,type_base_line=type_base_line,_n_steps=n_steps, type_data= type_data)\n\n                if not os.path.exists(biomarkers_folder):\n                    os.makedirs(biomarkers_folder)\n                    \n                biomakers_file_loc = biomarkers_folder + f'/IG_{type_base_line}_{type_data}_{num_models}_{data_folder.split(\"/\")[-1]}.csv'\n                pd_results[type_base_line][type_data][num_models][['gene_name']].to_csv(biomakers_file_loc, index=False)\n                list_biomakers_file_loc.append(biomakers_file_loc)\n\n    print()\n    return list_biomakers_file_loc","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:50:35.658711Z","iopub.execute_input":"2024-06-06T13:50:35.659081Z","iopub.status.idle":"2024-06-06T13:50:35.674200Z","shell.execute_reply.started":"2024-06-06T13:50:35.659043Z","shell.execute_reply":"2024-06-06T13:50:35.673424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_concerned_type_baseline = ['zeros', 'micro_means', 'macro_means', 'micro_mean__zero__micro_mean', 'macro_mean__zero__macro_mean', 'micro_mean__true_zero__micro_mean', 'macro_mean__true_zero__macro_mean', 'dict_default_micro_means', 'dict_default_macro_means']\n# list_concerned_type_data = ['tr','val']\n\n# list_concerned_type_baseline = ['zeros']\nlist_concerned_type_data = ['tr']\nlist_concerned_num_models = [num_models]\n\nn_steps=100\n\nlist_biomakers_file_loc = export_biomarker_file(list_concerned_type_baseline, list_concerned_type_data, list_concerned_num_models, biomarkers_folder, n_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:54:18.122292Z","iopub.execute_input":"2024-06-06T13:54:18.122670Z","iopub.status.idle":"2024-06-06T13:57:18.989376Z","shell.execute_reply.started":"2024-06-06T13:54:18.122635Z","shell.execute_reply":"2024-06-06T13:57:18.988472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3. MOGONET(NO IG)","metadata":{}},{"cell_type":"code","source":"if Run_MOGONET:\n    biomarker_file_name = f'mogonet_full_top_biomarkers_sorted_desc_score_{num_models}models.csv'\n    main_biomarker_mogonet = '/kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/main_biomarker.py'\n    topn=30\n\n    !python '{main_biomarker_mogonet}' '{data_folder}' '{model_folder}' '{dim_he_list}' '{view_list}' '{num_models}' '{postfix_tr}' '{postfix_te}' '{biomarkers_folder}' '{biomarker_file_name}' '{topn}'","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-06T13:53:35.478549Z","iopub.status.idle":"2024-06-06T13:53:35.478973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1. Check overlap with other genes","metadata":{}},{"cell_type":"code","source":"# biomarkers with direct evidence\nha_biomarker_lst=['H2AX','MGMT','IL1B','CXCL8','APC','HIF1A','VEGFA','MMP9','MMP2','MYC','MTOR','CDK6','WT1',\n                  'EGFR','GDNF','HES1','GSTT1','NDRG1','CTSB','TGM2','CD9','DUSP6','NOTCH1','MET','JAG1',\n                  'HEY1','PIM1','BMI1','MDM4','FAT1','FAM83D',\n                  'ERBB2', 'BRCA2',        'AKT3', 'ATRX', 'ZNF429']\ntopn = [50, 100, 200, 400]","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:57:59.072769Z","iopub.execute_input":"2024-06-06T13:57:59.073110Z","iopub.status.idle":"2024-06-06T13:57:59.078751Z","shell.execute_reply.started":"2024-06-06T13:57:59.073074Z","shell.execute_reply":"2024-06-06T13:57:59.077778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ge_feat_name = pd.read_csv(f'{data_folder}/1/1_featname.csv', header=None)[0].str.split(r'\\|').str[0]\nge_feat_name = list(set(ge_feat_name.tolist()))\n\ncna_feat_name = pd.read_csv(f'{data_folder}/1/2_featname.csv', header=None)[0].str.split(r'\\|').str[0]\ncna_feat_name = list(set(cna_feat_name.tolist()))\n\nmeth_feat_name = pd.read_csv(f'{data_folder}/1/3_featname.csv', header=None)[0].str.split(r'\\|').str[0]\nmeth_feat_name = list(set(meth_feat_name.tolist()))\n\ncombined_features = set(ge_feat_name + cna_feat_name + meth_feat_name)\nintersection_count = len(set(ha_biomarker_lst).intersection(combined_features))\n\nprint(intersection_count)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:53:35.481345Z","iopub.status.idle":"2024-06-06T13:53:35.481739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Run_MOGONET:\n    file_mogonet_biomarkers = biomarkers_folder + f'/mogonet_full_top_biomarkers_sorted_desc_score_{num_models}models.csv'\n    mogonet_biomarkers = pd.read_csv(file_mogonet_biomarkers).feat_name[:].tolist()\n\n    for i in topn:\n            print(f\"{file_mogonet_biomarkers.split('/')[-1]} top {i} INTERSECTION Ha's biomarker list:\\n\\t\", sorted(list(set(set(ha_biomarker_lst).intersection(set(mogonet_biomarkers[:i]))))))\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file_loc in list_biomakers_file_loc:\n    print(\"*\"*100)\n#     print(file_loc.split('/')[-1])\n#     print(\"\\n\"*2)\n    ig_biomarkers = pd.read_csv(file_loc).gene_name[:].str.split(r'\\|').str[0].values.tolist()#.to_list()\n    baseline_name = '-'.join(file_loc.split('/')[-1].split('_')[:-7])\n    for i in topn:\n        print(f\"{baseline_name} top {i} INTERSECTION Ha's biomarker list:\\n\\t\", sorted(list(set(set(ha_biomarker_lst).intersection(set(ig_biomarkers[:i]))))))\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:58:26.356889Z","iopub.execute_input":"2024-06-06T13:58:26.357234Z","iopub.status.idle":"2024-06-06T13:58:26.417299Z","shell.execute_reply.started":"2024-06-06T13:58:26.357205Z","shell.execute_reply":"2024-06-06T13:58:26.416393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2. TEST WITH CLASSIC ML ALGO","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:04.416391Z","iopub.execute_input":"2024-06-06T14:00:04.416763Z","iopub.status.idle":"2024-06-06T14:00:04.737110Z","shell.execute_reply.started":"2024-06-06T14:00:04.416732Z","shell.execute_reply":"2024-06-06T14:00:04.736358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Threshold to take number of genes (biomarkers) per subtype\n\nCOHORT = 'TCGA_GBM_METHxGExCNV_2000x2000x2000_MinMaxScaler'\nROOT_DATA_FOLDER = f'/kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/{COHORT}/train_test_split_org/'\nRANDOM_STATE = 42\n\nLIST_OMICS = ['GE', 'CNA', \"Meth\"]\nLIST_OMICS_ID = np.arange(1,len(LIST_OMICS)+1,1)\n\n# Single omic or Multi-omics| to run experiments\nLIST_EXP_OMICS = [\"GE_CNA_Meth\"]\n\n\nLIST_TYPE_DATA = ['train', 'test']\nDATA_FOLDER = {'train': ROOT_DATA_FOLDER,\n              'test': ROOT_DATA_FOLDER}\n\n\nORIGINAL_MAPPING_NAME = {\n    0:\"Classical\",\n    1:\"Mesenchymal\",\n    2:\"Neural\",\n    3:\"Proneural\"\n} ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:40.977921Z","iopub.execute_input":"2024-06-06T14:00:40.978289Z","iopub.status.idle":"2024-06-06T14:00:40.984522Z","shell.execute_reply.started":"2024-06-06T14:00:40.978238Z","shell.execute_reply":"2024-06-06T14:00:40.983408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nBIOMARKERS_RESULT_FOLDER = '/kaggle/working/biomarkers'\nlist_loc_biomarkers = []\nfor dirname, _, filenames in os.walk(BIOMARKERS_RESULT_FOLDER):\n    for filename in filenames:\n        list_loc_biomarkers.append(os.path.join(dirname, filename))\nprint(list_loc_biomarkers)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:07.286962Z","iopub.execute_input":"2024-06-06T14:00:07.287312Z","iopub.status.idle":"2024-06-06T14:00:07.293795Z","shell.execute_reply.started":"2024-06-06T14:00:07.287281Z","shell.execute_reply":"2024-06-06T14:00:07.292887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Markdown, display\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.expand_frame_repr', False)\n# pd.set_option('max_colwidth', None)\ndef printmd(string, color=None):\n    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n    display(Markdown(colorstr))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:09.134222Z","iopub.execute_input":"2024-06-06T14:00:09.134619Z","iopub.status.idle":"2024-06-06T14:00:09.139232Z","shell.execute_reply.started":"2024-06-06T14:00:09.134584Z","shell.execute_reply":"2024-06-06T14:00:09.138318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef display_classification_report(n_class\n                                  , label\n                                  , pred\n                                  , label_mapping_name\n                                  , cmap='Blues', fmt='.2%', annot=True\n                                  , path=None # str path to save fig. If not None\n                                 ):\n    display(pd.DataFrame({\n                            'ground_truth': {i:list(label).count(i) for i in np.arange(n_class)}.values()\n                            , 'pred': {i:list(pred).count(i) for i in np.arange(n_class)}.values()\n                         }\n                         , index=label_mapping_name))\n\n    \n    clf_report = classification_report(label, \n                                        pred, \n                                        target_names=label_mapping_name, \n                                        digits=4, \n                                        zero_division=0, \n                                        output_dict=True)\n\n    clf_df = pd.DataFrame(clf_report)\n    clf_df.loc[['precision', 'recall'],'accuracy']=np.nan\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_figwidth(12)\n    ConfusionMatrixDisplay(confusion_matrix(label, pred)\n                           , display_labels=label_mapping_name\n                          ).plot(cmap= cmap\n                                 , ax= ax1)\n    sns.heatmap(clf_df.iloc[:-1, :].T\n                , annot=annot\n                , cmap=cmap\n                , robust=True\n                , ax=ax2\n                , fmt=fmt)\n    plt.show()\n    if path is not None:\n        fig.savefig(path, dpi=300)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:11.444312Z","iopub.execute_input":"2024-06-06T14:00:11.444666Z","iopub.status.idle":"2024-06-06T14:00:11.519330Z","shell.execute_reply.started":"2024-06-06T14:00:11.444631Z","shell.execute_reply":"2024-06-06T14:00:11.518580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport pandas as pd\n\nimport copy\nfrom sklearn.compose import ColumnTransformer \n\ndef validate(biomarker_file, threshold):\n    # init var and para to save the result\n    ###\n    validate_result=[]\n    base_result = {}\n    ###\n#     dct_data_structure = {col: [] for col in lst_cols}\n#     pd_result = pd.DataFrame(columns=lst_cols)\n\n    # Read data\n    dict_df_label = {}\n    dict_df_data = {}\n    # Read data as df and create numpy array data for labeled data\n    for type_data in LIST_TYPE_DATA:\n        # modified\n        dict_df_label[type_data] = pd.read_csv(DATA_FOLDER[type_data] + f'labels_{type_data[:2]}.csv', names=['disease_subtypes'])\n        # ---------------------------------\n\n        # added\n        dict_df_label[type_data]['disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('int')\n        dict_df_label[type_data].index.names = ['sampleID']\n        \n        dict_df_label[type_data].replace({'disease_subtypes': ORIGINAL_MAPPING_NAME}, inplace=True)\n        # ---------------------------------\n        \n        \n        dict_df_omics = {}\n        dict_narray_omics = {}\n        for omic in LIST_OMICS:\n            # added\n            tmp_feat_name = pd.read_csv(DATA_FOLDER[type_data]+ f'{LIST_OMICS.index(omic)+1}_featname.csv', names=['feat_name'])\n#             tmp_feat_name['feat_name'] = tmp_feat_name['feat_name'].str.split('|').str[0]\n            lst_name = tmp_feat_name.values.reshape(-1).tolist()\n            # ---------------------------------\n\n            # modified\n            dict_df_omics[omic] = pd.read_csv(DATA_FOLDER[type_data] + f'{LIST_OMICS.index(omic)+1}_{type_data[:2]}.csv',names=lst_name)\n            # ---------------------------------\n\n        dict_df_data[type_data] = dict_df_omics\n\n    LABEL_MAPPING_NAME = dict_df_label['train']['disease_subtypes'].astype('category').cat.categories # sorted by alphabetical order\n    print('LABEL_MAPPING_NAME', LABEL_MAPPING_NAME)\n    \n    # Convert categorical label to numerical label\n    for type_data in LIST_TYPE_DATA:\n        dict_df_label[type_data].loc[:,'disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('category').cat.codes\n\n    #---------------------------------------------------------------------------------------\n    # Keep only biomarker genes found from TCGA data\n    print('-'*100)\n    print('KEEP ONLY BIOMARKER GENES FOUND FROM TCGA DATA')\n    score_genes = pd.read_csv(biomarker_file)\n    score_genes = score_genes.iloc[:threshold, 0]\n    top_genes = list(set(score_genes.to_numpy(copy=True).reshape(-1)))\n    print(top_genes)\n    top_genes = [gene.upper() for gene in top_genes]\n    print(f'Top {threshold} from TCGA have {len(top_genes)} unique genes/features:')\n    ###\n    base_result['n_unq_markers'] = len(top_genes)\n    base_result['lst_unq_markers'] = top_genes\n    ###\n    \n    GENE = {}\n    for omic in LIST_OMICS:\n        GENE[omic] = dict_df_data['train'][omic].columns[\n#             dict_df_data['train'][omic].columns.str.upper().str.split(r'\\|').str[0].isin(top_genes)\n            dict_df_data['train'][omic].columns.str.upper().isin(top_genes)\n        ].to_numpy(copy=True).tolist()\n\n        print(f'\\twith {omic} TOP {threshold}:', len(GENE[omic]))\n        ###\n        base_result[f'n_unq_{omic}'] = len(GENE[omic])\n        base_result[f'lst_unq_{omic}'] = GENE[omic]\n        ###\n    # NOTE THAT DNAmythyl and mRNA maybe have same genename in top gene => incresing num features comparing to num unique genes\n\n        for type_data in LIST_TYPE_DATA:\n            dict_df_data[type_data][omic] = dict_df_data[type_data][omic][GENE[omic]].copy(deep=True)\n    \n    dict_X = {}\n    dict_y = {}\n    for type_data in LIST_TYPE_DATA:\n        dict_X[type_data] = {}\n        dict_y[type_data] = {}\n\n    for type_omic in LIST_EXP_OMICS:\n        if '_' in type_omic:\n            print(f'Creating data for multi-omics experiment: {type_omic}')\n            list_omics = type_omic.split('_')\n            for type_data in LIST_TYPE_DATA:\n                tuple_data_omics = tuple([dict_df_data[type_data][single_omic] for single_omic in list_omics])\n                dict_X[type_data][type_omic] = np.concatenate(tuple_data_omics, axis=1)\n        else:\n            print(f'Creating data for single omic experiment: {type_omic}')\n            for type_data in LIST_TYPE_DATA:\n                dict_X[type_data][type_omic] = dict_df_data[type_data][type_omic].to_numpy(copy=True)\n\n        for type_data in LIST_TYPE_DATA:\n            dict_y[type_data][type_omic] = dict_df_label[type_data]['disease_subtypes'].to_numpy(copy=True)\n\n    def tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\n                        scoring, refit, is_binary_problem,\n                        result_on_dataset, rank_hparams_info):\n        assert 'test' in result_on_dataset\n        assert isinstance(rank_hparams_info, bool)\n        ###\n        lst_dct_result = [] # to return\n        ###\n        \n        start=datetime.now()\n        X = {}\n        y = {}\n        X['train'] = X_train\n        X['test'] = X_test\n        y['train'] = y_train\n        y['test'] = y_test\n\n        for model_name, gs_est in sorted(gridcvs.items()):\n            ###\n            sub_result = {}\n            sub_result['model'] = model_name\n            ###\n            \n            start_individual_type_model = datetime.now()\n            printmd(f'{model_name} classifier:\\n', color=\"blue\")\n            gs_est.fit(X['train'],y['train'])\n#             print(f'%s | best {refit} score  %.2f%% +/- %.2f' % \n#               (model_name, gs_est.best_score_ * 100, gs_est.cv_results_[f'std_test_{refit}'][gs_est.best_index_] * 100))\n            \n            ###\n            sub_result['best_params'] = gs_est.best_params_\n            ###\n            print('best params:', sub_result['best_params'])\n            \n            ###\n            sub_result[f'best_tuning_{refit}'] = gs_est.best_score_ * 100\n            sub_result['best_tuning_std'] = gs_est.cv_results_[f'std_test_{refit}'][gs_est.best_index_] * 100\n            ###\n            print(f'%s | best {refit} score  %.2f%% +/- %.2f' % \n              (model_name\n               , sub_result[f'best_tuning_{refit}']\n               ,  sub_result[f'best_tuning_std'])\n            )\n            \n            if rank_hparams_info:\n                print('\\n')\n                select_result_cols = []\n                for metric in scoring:\n                    select_result_cols.extend(['rank_test_'+metric,'mean_test_'+ metric, 'std_test_'+metric])\n                select_result_cols.extend(['params'])\n\n                dataframe_results = pd.DataFrame(gs_est.cv_results_).loc[:,select_result_cols].sort_values(by=f'mean_test_{refit}',ascending=False)\n                display(dataframe_results[:10])\n\n            for type_data in result_on_dataset:\n                print('\\n')\n                print(f'Result on {type_data} dataset with best hyperparameters:')\n                y_predict = gs_est.predict(X[type_data])\n\n                acc = accuracy_score(y_true=y[type_data], y_pred=y_predict)\n                \n                ###\n                sub_result[f'{type_data}_acc'] = acc * 100\n                ###\n                print(f'{type_data}_acc: {acc * 100:.2f}')\n                \n                if is_binary_problem:\n                    f1 = f1_score(y_true=y[type_data], y_pred=y_predict,average='binary')\n                    y_score = gs_est.predict_proba(X[type_data])[:, 1]\n                    roc_auc = roc_auc_score(y_true=y[type_data], y_score=y_score)\n                    \n                    ###\n                    sub_result[f'{type_data}_f1'] = f1 * 100\n                    ###\n                    print(f'{type_data}_f1: {f1 * 100:.2f}')\n                    \n                    ###\n                    sub_result[f'{type_data}_roc_auc'] = roc_auc * 100\n                    ###\n                    print(f'{type_data}_roc_auc: {roc_auc * 100:.2f}')\n                else:\n                    f1_macro = f1_score(y_true=y[type_data], y_pred=y_predict,average='macro')\n                    f1_weighted = f1_score(y_true=y[type_data], y_pred=y_predict,average='weighted')\n                    \n                    ###\n                    sub_result[f'{type_data}_f1_macro'] = f1_macro * 100\n                    ###\n                    print(f'{type_data}_f1_macro: {f1_macro * 100:.2f}')\n                    \n                    ###\n                    sub_result[f'{type_data}_f1_weighted'] = f1_weighted * 100\n                    ##\n                    print(f'{type_data}_f1_weighted: {f1_weighted * 100:.2f}')\n                \n                ###\n                lst_dct_result.append(sub_result)\n                ###\n                \n                print('ORIGINAL_MAPPING_NAME', ORIGINAL_MAPPING_NAME)\n                display(pd.DataFrame(classification_report(y_true=y[type_data], \n                                                           y_pred=y_predict,\n                                                           target_names=ORIGINAL_MAPPING_NAME.values(),\n                                                           digits=4, output_dict=True)))\n                pd_cfm = pd.crosstab(\n                    y[type_data]\n                    , y_predict\n                    , margins=True\n                    , rownames=['True label']\n                    , colnames=['Pred label']\n                )\n                pd_cfm.index = list(ORIGINAL_MAPPING_NAME.values()) + ['All']\n                pd_cfm.columns = list(ORIGINAL_MAPPING_NAME.values()) + ['All']\n                display(\n                   pd_cfm\n                )\n                \n                path_save_fig = '/kaggle/working/'+'cfm_' + f'{model_name}_'\\\n                    + f'top{threshold}_'\\\n                    + biomarker_file.split('/')[-1].split('.')[-2]\\\n                    + '.png'\n                display_classification_report(n_class=len(ORIGINAL_MAPPING_NAME)\n                                              , label=y[type_data]\n                                              , pred= y_predict\n                                              , label_mapping_name=ORIGINAL_MAPPING_NAME.values()\n                                              , path=path_save_fig\n                                             )\n                printmd(f'Total Time to Run {model_name} classifier: {datetime.now()-start_individual_type_model}','black')\n                print('-'*100)\n        print(f'Total Time: {datetime.now()-start}')\n        return lst_dct_result\n\n    def validate_biomarker(dict_X_train, dict_y_train, dict_X_test, dict_y_test,\n                           omics=['GE_CNA', 'GE','CNA'], random_state = RANDOM_STATE,\n                           result_on_dataset = ['train','test'], rank_hparams_info = True,\n                           is_binary_problem=False):\n        assert 'test' in result_on_dataset\n        assert isinstance(rank_hparams_info, bool)\n        ###\n        validate_biomarker_result = []\n        ###\n        scoring = None\n        refit= None\n        if is_binary_problem:\n            scoring = ['f1','accuracy','roc_auc']\n            refit = 'f1'\n        else: \n            scoring = ['f1_macro','f1_weighted', 'accuracy']\n            refit = 'f1_macro'\n\n        # Initializing classifiers\n        clf1 = LogisticRegression(random_state=random_state, max_iter=10000, n_jobs=-1)\n\n        # Binary case, probability = True to cal ROC_AUC, slowdown k-fold....\n        clf2 = SVC(random_state=random_state, probability=is_binary_problem)\n\n        clf3 = RandomForestClassifier(random_state=random_state,n_jobs=-1)\n        \n        # Building the pipelines\n        pipe1 = Pipeline([('std', 'passthrough'),\n                          ('clf1', clf1)])\n\n        pipe2 = Pipeline([('std', 'passthrough'),\n                          ('clf2', clf2)])\n        \n#         # only apply std to mRNA data/ BY index mRNA| ignore or passthorough not to\n#         # apply standard scaler to remaining index corresponding to CNA data\n#         column_trans = ColumnTransformer(\n#             [('scaler', StandardScaler(),list(range(len(GENE['mRNA']))))]\n#             ,remainder='passthrough')\n        # Setting up the parameter grids\n        param_grid1 = [{\n                        'std': [MinMaxScaler()],\n#                         'std': [StandardScaler()],\n#                         'std': [column_trans],\n                        'clf1__penalty': ['l2'],\n                        'clf1__multi_class':[\"multinomial\"],\n                        'clf1__solver':[\"newton-cg\"],\n                        'clf1__class_weight': [\"balanced\"],\n                        'clf1__C': np.power(10., np.arange(-4, 3)),\n                        }]\n\n        param_grid2 = [{\n                        'std': [MinMaxScaler()],\n#                         'std': [StandardScaler()],\n#                         'std': [column_trans],\n                        'clf2__kernel': ['rbf'],\n                        'clf2__class_weight': [\"balanced\"],\n                        'clf2__C': np.power(10., np.arange(-4, 3)),\n                        'clf2__gamma': list(np.power(10., np.arange(-4, 0))) + ['scale']\n                        }]\n\n        param_grid3 = [{'n_estimators': [50, 100, 150],\n                        'max_features': [\"sqrt\"],\n                        'max_depth' : list(range(1, 10)) + [None],\n                        'criterion' :[\"gini\"],\n                        'class_weight': [\"balanced\", \"balanced_subsample\"]}]\n\n        # Setting up multiple GridSearchCV objects, 1 for each algorithm\n        gridcvs = {}\n#         cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=10, random_state=random_state)\n        cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=5, random_state=random_state)\n\n        train_options = zip(\n                            (param_grid1\n                             ,param_grid2\\\n                             ,param_grid3\n                            ),\n                            (pipe1,pipe2\\\n                             ,clf3\n                            ),\n                            ('1_Softmax', '2_SVM'\\\n                             ,'3_RandomForest'\n                            )\n                           )\n\n        for pgrid, est, model_name in train_options:\n            gcv = GridSearchCV(estimator=est,\n                               param_grid=pgrid,\n                               scoring=scoring,\n                               n_jobs=-1,\n                               cv=cv,\n                               verbose=1,\n                               refit=refit)\n            gridcvs[model_name] = gcv\n\n        for omic in omics:\n            print('-'*100)\n            printmd(f'Validate on {omic} data:\\n', color=\"red\")\n\n            X_train = dict_X_train[omic]\n            y_train = np.array(dict_y_train[omic], dtype=np.int16)\n            print('Train dist: ', np.unique(y_train, return_counts=True ))\n\n            X_test = dict_X_test[omic]\n            y_test = np.array(dict_y_test[omic], dtype=np.int16)\n            print('Test dist', np.unique(y_test, return_counts=True ),'\\n')\n            \n            # run tuning and eval\n            tmp_lst_dct_tuning_result = tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\\\n                            scoring, refit,is_binary_problem,\n                            result_on_dataset, rank_hparams_info)\n            ###\n            tmp_base= {'using_omic': omic}\n            validate_biomarker_result.extend([copy.deepcopy(tmp_base) for i in range(len(tmp_lst_dct_tuning_result))])\n            for dct_tmp, dct_val in zip(tmp_lst_dct_tuning_result, validate_biomarker_result):\n                dct_val.update(dct_tmp)\n            ###\n        return validate_biomarker_result\n    tmp_lst_dct_validate_biomarker_result = validate_biomarker(dict_X['train'], dict_y['train'], dict_X['test'], dict_y['test'],\n                       omics=LIST_EXP_OMICS, random_state=RANDOM_STATE,\n                       result_on_dataset= ['test'], rank_hparams_info =False,\n                       is_binary_problem = (len(LABEL_MAPPING_NAME)==2))\n    validate_result.extend([copy.deepcopy(base_result) for i in range(len(tmp_lst_dct_validate_biomarker_result))])\n    for dct_tmp, dct_val in zip(tmp_lst_dct_validate_biomarker_result, validate_result):\n        dct_val.update(dct_tmp)\n    return validate_result","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:16.060902Z","iopub.execute_input":"2024-06-06T14:00:16.061234Z","iopub.status.idle":"2024-06-06T14:00:16.126098Z","shell.execute_reply.started":"2024-06-06T14:00:16.061204Z","shell.execute_reply":"2024-06-06T14:00:16.125250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESHOLD = 50\n# excluded_files = ['mogonet_full_top_biomarkers_sorted_desc_score_5models.csv']\n# excluded_files = [f'{BIOMARKERS_RESULT_FOLDER}/{COHORT}/'+biomarker_file for biomarker_file in excluded_files]\n\nexcluded_files = [] # evaluation all candidate biomarkers result","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:22.246670Z","iopub.execute_input":"2024-06-06T14:00:22.247006Z","iopub.status.idle":"2024-06-06T14:00:22.251036Z","shell.execute_reply.started":"2024-06-06T14:00:22.246977Z","shell.execute_reply":"2024-06-06T14:00:22.249950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.__version__)\nprint(sklearn.__version__)\nprint(np.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:23.588160Z","iopub.execute_input":"2024-06-06T14:00:23.588514Z","iopub.status.idle":"2024-06-06T14:00:23.593293Z","shell.execute_reply.started":"2024-06-06T14:00:23.588484Z","shell.execute_reply":"2024-06-06T14:00:23.592358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# # Ignore ConvergenceWarning\n# warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n\n##\nresult = []\nTHRESHOLD_LST = [20, 50, 100, 150, 200, 250, 300, 350, 400]\n##\n\nfor threshold in THRESHOLD_LST:\n    for biomarker_file in list_loc_biomarkers:\n        if biomarker_file in excluded_files:\n            continue\n        print(\"*\"*100)\n        baseline = biomarker_file.split('/')[-1].split('.')[-2]\n        printmd(baseline,'green')\n        print(biomarker_file)\n\n        ###\n        tmp_result = []\n        base_init_dct_result = {'top': threshold, 'baseline': baseline}\n        tmp_validate_result = validate(biomarker_file,threshold)\n        tmp_result.extend([copy.deepcopy(base_init_dct_result) for i in range(len(tmp_validate_result))])\n        for dct_tmp, dct_val in zip(tmp_validate_result, tmp_result):\n            dct_val.update(dct_tmp)\n\n        result.extend(tmp_result)\n        ###\n        print(f\"Top {threshold} - Using {tmp_result[0]['n_unq_markers']} uniques biomarkers in totals\")\n        print(tmp_result[0]['lst_unq_markers'])\n        print('\\n'*2)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:00:47.712021Z","iopub.execute_input":"2024-06-06T14:00:47.712435Z","iopub.status.idle":"2024-06-06T14:06:27.515258Z","shell.execute_reply.started":"2024-06-06T14:00:47.712400Z","shell.execute_reply":"2024-06-06T14:06:27.514347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3. Compare all IG baselines and MOGONET","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\navg_acc_all_models = {}\navg_f1_all_models = {}\n\nfor biomarker_file in list_loc_biomarkers:\n    ### Extract accuracy and F1 scores for plotting\n    for res in result:\n#         print(res)\n        model_type = res['model']\n        baseline_type = \"-\".join(res['baseline'].split('_')[:-7])\n        top_n = res['top']\n        accuracy_t = res['test_acc']\n        f1_score_t = res.get('test_f1_macro', res.get('test_f1', 0))\n\n        if model_type not in avg_acc_all_models:\n            avg_acc_all_models[model_type] = {}\n            avg_f1_all_models[model_type] = {}\n        \n        if baseline_type not in avg_acc_all_models[model_type]:\n            avg_acc_all_models[model_type][baseline_type] = []\n            avg_f1_all_models[model_type][baseline_type] = []\n        \n        avg_acc_all_models[model_type][baseline_type].append((top_n, accuracy_t))\n        avg_f1_all_models[model_type][baseline_type].append((top_n, f1_score_t))\n        \nfor model_type in avg_acc_all_models:\n    for baseline_type in avg_acc_all_models[model_type]:\n        avg_acc_all_models[model_type][baseline_type].sort(key=lambda x: x[0])\n        avg_f1_all_models[model_type][baseline_type].sort(key=lambda x: x[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:53:35.496080Z","iopub.status.idle":"2024-06-06T13:53:35.496519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metrics(metrics, metric_name):\n    path_save_fig = '/kaggle/working/' + 'compare_' + f'{metric_name}' + '.png'\n    plt.figure(figsize = (10,6), dpi=300)\n    for baseline_name, values in metrics.items():\n        top_n_list = [item[0] for item in values]\n        metric_values = [item[1] for item in values]\n        plt.plot(top_n_list, metric_values, marker='o', label=baseline_name)\n    plt.title(f'{metric_name} for different baselines and top N features')\n    plt.xlabel('Top N features')\n    plt.ylabel(metric_name)\n    plt.legend(fontsize='small', bbox_to_anchor=(1.05, 1), loc='upper left')    \n    plt.xticks(top_n_list)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(path_save_fig)\n    plt.show()\n    \n# Plot accuracy\nfor model_type in avg_acc_all_models.keys():\n    print('*' * 20, model_type, '*' * 20)\n    plot_metrics(avg_acc_all_models[model_type], f'Accuracy for {model_type}')\n    plot_metrics(avg_f1_all_models[model_type], f'F1 Score for {model_type}')\n    print('\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T13:53:35.497304Z","iopub.status.idle":"2024-06-06T13:53:35.497715Z"},"trusted":true},"execution_count":null,"outputs":[]}]}